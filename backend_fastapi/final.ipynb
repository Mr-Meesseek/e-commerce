{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def removeDiacritics(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text.lower()) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemin du dossier où sauvegarder le fichier\n",
    "output_folder = \"./translated_data\"\n",
    "\n",
    "# Créer le dossier s'il n'existe pas déjà\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Charger le fichier JSON avec les données originales\n",
    "with open(\"anc.json\", encoding=\"utf8\") as file:\n",
    "    discussions = json.load(file)\n",
    "\n",
    "# Charger le fichier Excel et créer un dictionnaire de traduction\n",
    "data = pd.read_excel('./Book1.xlsx')\n",
    "# Replace NaN values with a placeholder or remove rows with NaN in relevant columns\n",
    "data = data.dropna(subset=['Column3', 'Column5', 'Column8'])\n",
    "data['Column3'].apply(removeDiacritics)\n",
    "data['Column5'].apply(removeDiacritics)\n",
    "data['Column8'].apply(removeDiacritics)\n",
    "arabish_coda_dict = dict(zip(data['Column3'], data['Column5'].astype(str)))\n",
    "normm_lemma = dict(zip(data['Column5'].astype(str), data['Column8'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_arabic_with_coda(tokens, arabish_coda_dict):\n",
    "#     \"\"\"Traduire les tokens en utilisant le dictionnaire.\"\"\"\n",
    "#     new_tokens = []\n",
    "#     for token in tokens:\n",
    "#         if token in arabish_coda_dict:\n",
    "#             new_tokens.append(arabish_coda_dict[token])\n",
    "#         else:\n",
    "#             new_tokens.append(token)\n",
    "#     return new_tokens\n",
    "\n",
    "def replace_arabic_with_coda(tokens, arabish_coda_dict):\n",
    "    \"\"\"Traduire les tokens en utilisant le dictionnaire.\"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        token = removeDiacritics(token)\n",
    "        new_tokens.append(arabish_coda_dict.get(token, token))\n",
    "    return new_tokens\n",
    "\n",
    "# def normalize_tokens(tokens, normm_lemma):\n",
    "#     \"\"\"Normaliser les tokens en utilisant le dictionnaire.\"\"\"\n",
    "#     new_tokens = []\n",
    "#     for token in tokens:\n",
    "#         if token in normm_lemma:\n",
    "#             new_tokens.append(normm_lemma[token])\n",
    "#         else:\n",
    "#             new_tokens.append(token)\n",
    "#     return new_tokens\n",
    "def normalize_tokens(tokens, normm_lemma):\n",
    "    \"\"\"Normaliser les tokens en utilisant le dictionnaire.\"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_tokens.append(normm_lemma.get(token, token))\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "# def normalize_tokens(tokens):\n",
    "    \n",
    "#     # Créer un dictionnaire de correspondance clé-valeur à partir des colonnes 5 et 8\n",
    "#     mapping = dict(zip(data['Column5'], data['Column8']))\n",
    "    \n",
    "#     # Normaliser les tokens en utilisant le dictionnaire de correspondance\n",
    "#     normalized_tokens = [mapping.get(token, token) for token in tokens]\n",
    "    \n",
    "#     return normalized_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Traiter chaque discussion pour traduire les questions et les réponses\n",
    "for discussion in discussions:\n",
    "    for utterance in discussion:\n",
    "      \n",
    "      # Traduction de la question\n",
    "        question = removeDiacritics(utterance[\"question\"])\n",
    "        question_tokens = re.findall(r\"(\\w+|[^\\s]+)\", question)\n",
    "        translated_question_tokens = replace_arabic_with_coda(question_tokens, arabish_coda_dict)\n",
    "        utterance[\"question\"] = \" \".join(map(str, translated_question_tokens))  # Ensure all items are strings\n",
    "        \n",
    "        \n",
    "        # Normalisation de la question\n",
    "        normalized_question_tokens = normalize_tokens(translated_question_tokens, normm_lemma)\n",
    "        utterance[\"question_norm\"] = \" \".join(map(str, normalized_question_tokens))  # Ensure all items are strings\n",
    "\n",
    "        # Traduction de la réponse\n",
    "        answer = removeDiacritics(utterance[\"reponse\"])\n",
    "        answer_tokens = re.findall(r\"(\\w+|[^\\s]+)\", answer)\n",
    "        translated_answer_tokens = replace_arabic_with_coda(answer_tokens, arabish_coda_dict)\n",
    "        utterance[\"reponse\"] = \" \".join(map(str, translated_answer_tokens))  # Ensure all items are strings\n",
    "\n",
    "        # Affichage des résultats pour vérification\n",
    "        #print(\"Translated Question:\", utterance[\"question\"])\n",
    "        #print(\"Translated Question Norm:\", utterance[\"question_norm\"])\n",
    "        #print(\"Translated Answer:\", utterance[\"reponse\"])\n",
    "# for discussion in discussions:\n",
    "#     for utterance in discussion:\n",
    "#         # Traduction de la question\n",
    "#         question = utterance[\"question\"].lower()\n",
    "#         question_tokens = re.findall(r\"(\\w+|[^\\s]+)\", question)\n",
    "#         translated_question_tokens = replace_arabic_with_coda(question_tokens, arabish_coda_dict)\n",
    "#         normalized_question_tokens = normalize_tokens(translated_question_tokens, normm_lemma)\n",
    "#         utterance[\"question\"] = \" \".join(normalized_question_tokens)\n",
    "\n",
    "#         # Traduction de la réponse\n",
    "#         answer = utterance[\"reponse\"].lower()\n",
    "#         answer_tokens = re.findall(r\"(\\w+|[^\\s]+)\", answer)\n",
    "#         translated_answer_tokens = replace_arabic_with_coda(answer_tokens, arabish_coda_dict)\n",
    "#         normalized_response_tokens = normalize_tokens(translated_answer_tokens, normm_lemma)\n",
    "#         utterance[\"reponse\"] = \" \".join(normalized_response_tokens)\n",
    "\n",
    "#         # Affichage des résultats pour vérification\n",
    "#         print(\"Translated Question:\", utterance[\"question\"])\n",
    "#         print(\"Translated Answer:\", utterance[\"reponse\"])\n",
    "        \n",
    "\n",
    "    # Utiliser les questions et réponses traduites et normalisées pour le traitement ultérieur\n",
    "    # ...\n",
    "        \n",
    "        # updates.append((translated_question_tokens + translated_answer_tokens, q_pos + a_pos, q_lemmas + a_lemmas))\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les modifications dans un nouveau fichier JSON dans le dossier créé\n",
    "output_path = \"./res.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf8\") as file:\n",
    "    json.dump(discussions, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def read_documents(file_path):\n",
    "    discussions = json.load(open(file_path, 'r', encoding='utf-8'))\n",
    "    \n",
    "    #for i, discussion in enumerate(discussions):\n",
    "    #    for j, itturance in enumerate(discussion):\n",
    "    #        documents[f\"{i}_{j}\"] = itturance\n",
    "    return discussions\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set(set1).intersection(set(set2)))\n",
    "    union = len(set(set1).union(set(set2)))\n",
    "    return intersection / union\n",
    "\n",
    "# S1 : nheb ne5edh 9ardh -> hab 5dhe 9ardh\n",
    "# S2 : nheb ne5ou 9arth -> hab 5dhe 9ardh\n",
    "def search(query, discussions, normm_lemma):\n",
    "    \"\"\"Search and return top 5 similar documents.\"\"\"\n",
    "    normalized_query = normalize_tokens(re.findall(r\"(\\w+|\\S+)\", removeDiacritics(query)), normm_lemma)\n",
    "    results = []\n",
    "    for disc_id, discussion in enumerate(discussions):\n",
    "        for utter_id, utterance in enumerate(discussion):\n",
    "            doc_tokens = normalize_tokens(re.findall(r\"(\\w+|\\S+)\", utterance[\"question_norm\"].lower()), normm_lemma)\n",
    "            similarity = jaccard_similarity(set(normalized_query), set(doc_tokens))\n",
    "            results.append((disc_id, utter_id, similarity))\n",
    "    # Sort results by similarity score in descending order and return top 5\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results[:5]\n",
    "\n",
    "file_path = './res.json'  # Adjust this to the correct path\n",
    "excelpath='./Book1.xlsx'\n",
    "# Load the documents\n",
    "discussions = read_documents(file_path)\n",
    "query = \"انا عامل مترسم في شركة خاصةوماذبيا على قرض بدون فائض\" \n",
    "\n",
    "def prevUtterances(utter_id):\n",
    "    prev_utterances = []\n",
    "    for utterance in discussions[disc_id][utter_id-2:utter_id]:\n",
    "        prev_utterances.append(utterance[\"question\"])\n",
    "        prev_utterances.append(utterance[\"reponse\"])\n",
    "    return prev_utterances\n",
    "\n",
    "def topReponses(query, discussions, normm_lemma):\n",
    "    # Perform the search and get the top 5 results\n",
    "    search_results = search(query, discussions, normm_lemma)\n",
    "    reponses = []\n",
    "    for result in search_results:\n",
    "        disc_id, utter_id, similarity = result\n",
    "        reponses.append(discussions[disc_id][utter_id][\"reponse\"])\n",
    "        #question = discussions[disc_id][utter_id][\"question\"]\n",
    "        \n",
    "        #print(f\"Discussion[{disc_id}, {utter_id}] - Similarity: {similarity}\")\n",
    "        #print(f\"Question: {question}\")\n",
    "        #print(f\"Reponse: {reponse}\")\n",
    "        #print(\"\\n\\n\")\n",
    "    return reponses\n",
    "\n",
    "# Dataset avec le contexte et les dernieres utterances\n",
    "dataset = {\"data\": [{\"paragraphs\": [], \"title\": \"1\"}], \"version\": \"1.1\"}\n",
    "\n",
    "id_qas = 0\n",
    "for disc_id, discussion in enumerate(discussions):\n",
    "    for utter_id, utterance in enumerate(discussion):\n",
    "\n",
    "        # prev utterances\n",
    "        prev_utterances = prevUtterances(utter_id)\n",
    "        # top reponses\n",
    "        top_reponses = topReponses(utterance[\"question\"], discussions, normm_lemma)\n",
    "\n",
    "        # 6 echantillons: 5 avec la reponse correcte en premier, 1 avec reponses mélangées\n",
    "        for i in range(6):\n",
    "\n",
    "            # mélanger les reponses?\n",
    "            answer_start = 0\n",
    "            if i>0:\n",
    "                # Melanger reponses et trouver le nv answer_index\n",
    "                newL = list(enumerate(top_reponses))\n",
    "                random.shuffle(newL)\n",
    "                top_reponses_shuffled = []\n",
    "                trouve_correct = False\n",
    "                for old_j, rep in newL:\n",
    "                    top_reponses_shuffled.append(top_reponses[old_j])\n",
    "\n",
    "                    # retrouvé la reponse correcte?\n",
    "                    if old_j==0:\n",
    "                        trouve_correct = True\n",
    "                    elif not trouve_correct:\n",
    "                        answer_start += len(rep) + 3\n",
    "\n",
    "                top_reponses = top_reponses_shuffled\n",
    "            id_qas += 1\n",
    "            context = \" ; \".join(top_reponses) + \" ;\"\n",
    "            question = \"\\n\".join(prev_utterances) + \"\\n\" + utterance[\"question\"]\n",
    "            answer_text = utterance[\"reponse\"]\n",
    "\n",
    "            # remplir le dataset\n",
    "            dataset[\"data\"][0][\"paragraphs\"].append(\n",
    "                {\n",
    "                    \"context\": context,\n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"id\": str(id_qas),\n",
    "                            \"question\": question,\n",
    "                            \"answers\": [{\"answer_start\": answer_start, \"text\": answer_text}]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "output_path = \"./dataset.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf8\") as file:\n",
    "    json.dump(dataset, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
